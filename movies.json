name: Perfect Movie Crawler

on:
  schedule:
    - cron: '0 */6 * * *'  # 6ì‹œê°„ë§ˆë‹¤
  workflow_dispatch:  # ìˆ˜ë™ ì‹¤í–‰ ê°€ëŠ¥

jobs:
  crawl-perfect-movie-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 lxml urllib3
        
    - name: Perfect movie crawling
      env:
        KOBIS_KEY: ${{ secrets.KOBIS_API_KEY }}
      run: |
        python << 'EOF'
        import requests
        import json
        from datetime import datetime, timedelta
        import os
        import time
        import re
        from bs4 import BeautifulSoup
        from urllib.parse import quote, urljoin
        import urllib.parse

        def clean_text(text):
            """í…ìŠ¤íŠ¸ ì •ë¦¬"""
            if not text:
                return ""
            return re.sub(r'\s+', ' ', text.strip())

        def get_naver_movie_perfect(movie_title, open_year=None):
            """ë„¤ì´ë²„ ì˜í™”ì—ì„œ ì™„ë²½ í¬ë¡¤ë§"""
            try:
                print(f"ğŸ” ë„¤ì´ë²„ì—ì„œ '{movie_title}' ì™„ë²½ ê²€ìƒ‰ ì¤‘...")
                
                # ë„¤ì´ë²„ ì˜í™” ê²€ìƒ‰ (ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜)
                search_query = f"ì˜í™” {movie_title} ì •ë³´"
                search_url = "https://search.naver.com/search.naver"
                params = {
                    'where': 'nexearch',
                    'sm': 'tab_etc',
                    'mra': 'bkEw',
                    'pkid': '68',
                    'query': search_query
                }
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                }
                
                # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€
                response = requests.get(search_url, params=params, headers=headers, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # í¬ìŠ¤í„° ì´ë¯¸ì§€ ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: .thumb ._item img)
                poster_url = ""
                poster_link = soup.find('a', class_='thumb _item')
                if poster_link:
                    img_tag = poster_link.find('img', class_='_img')
                    if img_tag and img_tag.get('src'):
                        poster_url = img_tag['src']
                        # ê³ í™”ì§ˆ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                        if 'search.pstatic.net' in poster_url:
                            poster_url = poster_url.replace('&size=176x264', '&size=300x450')
                
                # ì¤„ê±°ë¦¬ ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: .intro_box ._content .text._content_text)
                plot = ""
                intro_box = soup.find('div', class_='intro_box _content')
                if intro_box:
                    plot_p = intro_box.find('p', class_='text _content_text')
                    if plot_p:
                        plot = clean_text(plot_p.get_text())
                
                # ì˜ì–´ì œëª© ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: .sub_title)
                english_title = ""
                sub_title = soup.find('div', class_='sub_title')
                if sub_title:
                    spans = sub_title.find_all('span', class_='txt')
                    for span in spans:
                        text = clean_text(span.get_text())
                        # ì˜ì–´ë¡œ ëœ ì œëª© ì°¾ê¸°
                        if re.match(r'^[A-Za-z\s:\'\.]+$', text) and text != 'ì˜í™”':
                            english_title = text
                            break
                
                # ìƒì„¸ ì •ë³´ ì¶”ì¶œ (ì‹¤ì œ êµ¬ì¡°: .detail_info .info.txt_4)
                info_area = soup.find('dl', class_='info txt_4')
                genre = ""
                runtime = ""
                grade = ""
                release_date = ""
                country = ""
                distributor = ""
                
                if info_area:
                    info_groups = info_area.find_all('div', class_='info_group')
                    for group in info_groups:
                        dt = group.find('dt')
                        dd = group.find('dd')
                        if dt and dd:
                            dt_text = clean_text(dt.get_text()).replace('|', '').strip()
                            dd_text = clean_text(dd.get_text())
                            
                            if 'ê°œë´‰' in dt_text:
                                release_date = dd_text
                            elif 'ë“±ê¸‰' in dt_text:
                                grade = dd_text
                            elif 'ì¥ë¥´' in dt_text:
                                genre = dd_text
                            elif 'êµ­ê°€' in dt_text:
                                country = dd_text
                            elif 'ëŸ¬ë‹íƒ€ì„' in dt_text:
                                runtime = dd_text
                            elif 'ë°°ê¸‰' in dt_text:
                                distributor = dd_text
                
                # ê°ë… ì •ë³´ëŠ” ì¶œì—°/ì œì‘ì§„ íƒ­ì—ì„œ ê°€ì ¸ì™€ì•¼ í•˜ë¯€ë¡œ ê¸°ë³¸ê°’
                director = "ì •ë³´ ì¤€ë¹„ ì¤‘"
                
                result = {
                    'poster_url': poster_url,
                    'plot': plot or "ì¤„ê±°ë¦¬ ì •ë³´ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.",
                    'genre': genre or "ì •ë³´ ì¤€ë¹„ ì¤‘",
                    'director': director,
                    'runtime': runtime or "ì •ë³´ ì¤€ë¹„ ì¤‘",
                    'grade': grade or "ì •ë³´ ì¤€ë¹„ ì¤‘",
                    'english_title': english_title,
                    'release_date': release_date,
                    'country': country,
                    'distributor': distributor
                }
                
                print(f"âœ… '{movie_title}' ë„¤ì´ë²„ ì™„ë²½ í¬ë¡¤ë§ ì™„ë£Œ")
                print(f"   í¬ìŠ¤í„°: {'ìˆìŒ' if poster_url else 'ì—†ìŒ'}")
                print(f"   ì¤„ê±°ë¦¬: {'ìˆìŒ' if plot else 'ì—†ìŒ'}")
                print(f"   ì¥ë¥´: {genre}")
                print(f"   ì˜ì–´ì œëª©: {english_title}")
                print(f"   ê°œë´‰ì¼: {release_date}")
                print(f"   ë“±ê¸‰: {grade}")
                print(f"   ëŸ¬ë‹íƒ€ì„: {runtime}")
                
                return result
                
            except Exception as e:
                print(f"âŒ '{movie_title}' ë„¤ì´ë²„ ì™„ë²½ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                return get_default_movie_data()

        def get_director_info(movie_title):
            """ê°ë… ì •ë³´ ë³„ë„ ìˆ˜ì§‘"""
            try:
                # ê°ë… ì •ë³´ë¥¼ ìœ„í•œ ë³„ë„ ê²€ìƒ‰
                search_query = f"{movie_title} ê°ë…"
                search_url = "https://search.naver.com/search.naver"
                params = {
                    'where': 'nexearch',
                    'sm': 'top_hty',
                    'fbm': '0',
                    'ie': 'utf8',
                    'query': search_query
                }
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                }
                
                response = requests.get(search_url, params=params, headers=headers, timeout=10)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ê°„ë‹¨í•œ ê°ë… ì •ë³´ ì¶”ì¶œ ì‹œë„
                director = "ì •ë³´ ì¤€ë¹„ ì¤‘"
                
                # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê°ë… ì •ë³´ ì°¾ê¸°
                for text_element in soup.find_all(string=True):
                    text = clean_text(str(text_element))
                    if 'ê°ë…' in text and len(text) < 50:
                        # ê°ë… ì´ë¦„ ì¶”ì¶œ íŒ¨í„´
                        director_match = re.search(r'ê°ë…[:\s]*([ê°€-í£]{2,4})', text)
                        if director_match:
                            director = director_match.group(1)
                            break
                
                return director
                
            except Exception as e:
                print(f"ê°ë… ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                return "ì •ë³´ ì¤€ë¹„ ì¤‘"

        def get_default_movie_data():
            """ê¸°ë³¸ ì˜í™” ë°ì´í„°"""
            return {
                'poster_url': "",
                'plot': "ì¤„ê±°ë¦¬ ì •ë³´ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.",
                'genre': "ì •ë³´ ì¤€ë¹„ ì¤‘",
                'director': "ì •ë³´ ì¤€ë¹„ ì¤‘",
                'runtime': "ì •ë³´ ì¤€ë¹„ ì¤‘",
                'grade': "ì •ë³´ ì¤€ë¹„ ì¤‘",
                'english_title': "",
                'release_date': "",
                'country': "",
                'distributor': ""
            }

        def main():
            print('ğŸ¬ ì‹¤ì œ ë„¤ì´ë²„ êµ¬ì¡° ê¸°ë°˜ ì™„ë²½ í¬ë¡¤ë§ ì‹œì‘...')
            
            # KOBIS API í˜¸ì¶œ
            kobis_key = os.environ['KOBIS_KEY']
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
            
            print(f'ğŸ“… ì¡°íšŒ ë‚ ì§œ: {yesterday}')
            
            kobis_url = "http://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.json"
            kobis_params = {
                'key': kobis_key,
                'targetDt': yesterday,
                'itemPerPage': 10
            }
            
            kobis_response = requests.get(kobis_url, params=kobis_params, timeout=15)
            kobis_response.raise_for_status()
            kobis_data = kobis_response.json()
            
            movies = kobis_data['boxOfficeResult']['dailyBoxOfficeList']
            print(f'ğŸ“Š KOBISì—ì„œ {len(movies)}ê°œ ì˜í™” ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ')
            
            # ê° ì˜í™”ì— ëŒ€í•´ ì™„ë²½ í¬ë¡¤ë§
            enhanced_movies = []
            
            for i, movie in enumerate(movies):
                print(f'\nğŸ•·ï¸ {i+1}/{len(movies)} - {movie["movieNm"]} ì™„ë²½ í¬ë¡¤ë§ ì¤‘...')
                
                movie_title = movie['movieNm']
                
                # ë„¤ì´ë²„ì—ì„œ ì™„ë²½ í¬ë¡¤ë§
                movie_details = get_naver_movie_perfect(movie_title)
                time.sleep(3)  # í¬ë¡¤ë§ ê°„ê²© (ì¤‘ìš”!)
                
                # ê°ë… ì •ë³´ ë³„ë„ ìˆ˜ì§‘
                if movie_details['director'] == "ì •ë³´ ì¤€ë¹„ ì¤‘":
                    director_info = get_director_info(movie_title)
                    movie_details['director'] = director_info
                    time.sleep(2)
                
                # ê¸°ì¡´ KOBIS ë°ì´í„°ì— í¬ë¡¤ë§ ì •ë³´ ì¶”ê°€
                enhanced_movie = {
                    **movie,  # ê¸°ì¡´ KOBIS ë°ì´í„° ìœ ì§€
                    **movie_details  # ì™„ë²½ í¬ë¡¤ë§í•œ ìƒì„¸ ì •ë³´ ì¶”ê°€
                }
                
                enhanced_movies.append(enhanced_movie)
                print(f'âœ… {movie_title} ì™„ë£Œ')
            
            # ìµœì¢… ë°ì´í„° ìƒì„±
            final_data = {
                'updateTime': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'targetDate': yesterday,
                'totalMovies': len(enhanced_movies),
                'dataSource': 'KOBIS + ë„¤ì´ë²„ì˜í™” ì‹¤ì œêµ¬ì¡° ì™„ë²½í¬ë¡¤ë§',
                'crawlingMethod': 'ì‹¤ì œ ë„¤ì´ë²„ ì˜í™” í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ê¸°ë°˜',
                'movies': enhanced_movies
            }
            
            # JSON íŒŒì¼ ì €ì¥
            with open('movies.json', 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
                
            print(f'\nğŸ‰ ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜ ì™„ë²½ í¬ë¡¤ë§ ì™„ë£Œ!')
            print(f'ğŸ“Š ì´ {len(enhanced_movies)}ê°œ ì˜í™”')
            print(f'ğŸ–¼ï¸ í¬ìŠ¤í„° ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("poster_url"))}ê°œ')
            print(f'ğŸ“ ì¤„ê±°ë¦¬ ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("plot") and "ì¤€ë¹„ ì¤‘" not in m["plot"])}ê°œ')
            print(f'ğŸ­ ì¥ë¥´ ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("genre") and "ì¤€ë¹„ ì¤‘" not in m["genre"])}ê°œ')
            print(f'ğŸ¬ ê°ë… ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("director") and "ì¤€ë¹„ ì¤‘" not in m["director"])}ê°œ')
            
            # ìƒ˜í”Œ ë°ì´í„° ì¶œë ¥
            if enhanced_movies:
                sample = enhanced_movies[0]
                print(f'\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„° ({sample["movieNm"]}):')
                print(f'   í¬ìŠ¤í„°: {sample.get("poster_url", "ì—†ìŒ")[:80]}...')
                print(f'   ì¤„ê±°ë¦¬: {sample.get("plot", "ì—†ìŒ")[:80]}...')
                print(f'   ì¥ë¥´: {sample.get("genre", "ì—†ìŒ")}')
                print(f'   ê°ë…: {sample.get("director", "ì—†ìŒ")}')
                print(f'   ë“±ê¸‰: {sample.get("grade", "ì—†ìŒ")}')
                print(f'   ëŸ¬ë‹íƒ€ì„: {sample.get("runtime", "ì—†ìŒ")}')
                print(f'   ì˜ì–´ì œëª©: {sample.get("english_title", "ì—†ìŒ")}')
            
        if __name__ == "__main__":
            main()
        EOF
        
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Perfect Movie Crawler"
        git add movies.json
        if git diff --staged --quiet; then
          echo "ë³€ê²½ì‚¬í•­ ì—†ìŒ"
        else
          git commit -m "ğŸ•·ï¸ ì‹¤ì œ ë„¤ì´ë²„ êµ¬ì¡° ê¸°ë°˜ ì™„ë²½ í¬ë¡¤ë§ $(date +'%Y-%m-%d %H:%M')"
          git push
          echo "âœ… ì™„ë²½ í¬ë¡¤ë§ ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ"
        fi
