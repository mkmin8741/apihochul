name: IPO Crawler

on:
  schedule:
    - cron: '0 */2 * * *'
  workflow_dispatch:

jobs:
  final-ipo-crawler:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 lxml
        
    - name: Crawl IPO Information (Final Version)
      run: |
        python << 'EOF'
        import requests
        import json
        from datetime import datetime
        from bs4 import BeautifulSoup
        from urllib.parse import urljoin
        import copy

        def get_text_without_children(tag, child_to_remove):
            d = copy.copy(tag)
            if d.find(child_to_remove):
                d.find(child_to_remove).decompose()
            return d.get_text(strip=True)

        def crawl_final_ipo_data():
            try:
                print("🏢 최종 IPO 정보 크롤링 시작...")
                base_url = "https://finance.n.com" # 실제 URL은 숨김 처리
                url = "https://finance.naver.com/sise/ipo.naver" # 크롤링 대상 URL
                
                headers = {'User-Agent': 'Mozilla/5.0'}
                response = requests.get(url, headers=headers, timeout=20)
                response.encoding = 'euc-kr'

                if response.status_code != 200:
                    print(f"❌ HTTP Error: {response.status_code}")
                    return []

                soup = BeautifulSoup(response.text, 'lxml')
                ipo_list = []
                rows = soup.select('table.type_7 > tbody > tr')

                for row in rows:
                    item_area = row.find('div', class_='item_area')
                    if not item_area: continue

                    company_name_tag = item_area.find('h4', class_='item_name')
                    company_name = company_name_tag.find('a').get_text(strip=True)
                    market_type = company_name_tag.find('span', class_='type').get_text(strip=True)
                    
                    info = {'company_name': company_name, 'market_type': market_type}
                    info_list = item_area.find('ul', class_='lst_info')

                    if info_list:
                        price_li = info_list.find('li', class_='area_price')
                        info['offer_price'] = price_li.find('span', class_='num').get_text(strip=True).replace('~', ' ~ ') if price_li and price_li.find('span', class_='num') else '미정'
                        
                        type_li = info_list.find('li', class_='area_type')
                        info['industry'] = get_text_without_children(type_li, 'em') if type_li else '미정'

                        sup_li = info_list.find('li', class_='area_sup')
                        info['underwriters'] = get_text_without_children(sup_li, 'em') if sup_li else '미정'

                        comp_li = info_list.find('li', class_='area_competition')
                        info['competition_rate'] = comp_li.find('span', class_='num').get_text(strip=True) if comp_li and comp_li.find('span', class_='num') else '-'

                        state_li = info_list.find('li', class_='area_state')
                        info['status'] = get_text_without_children(state_li, 'div') if state_li else '정보없음'

                        private_li = info_list.find('li', class_='area_private')
                        info['subscription_period'] = private_li.find('span', class_='num').get_text(strip=True).replace('~', ' ~ ') if private_li and private_li.find('span', 'num') else '미정'
                        
                        list_li = info_list.find('li', class_='area_list')
                        info['listing_date'] = list_li.find('span', 'num').get_text(strip=True) if list_li and list_li.find('span', 'num') else '미정'

                    docs_cell = row.find_all('td')[-1]
                    info['documents'] = []
                    if docs_cell:
                        for link in docs_cell.find_all('a', class_='lst'):
                            doc_type = '기업개요' if '기업개요' in link.get_text(strip=True) else 'IR BOOK'
                            info['documents'].append({'type': doc_type, 'url': link['href']})
                    
                    ipo_list.append(info)
                    print(f"   ✅ '{company_name}' 정보 수집 완료")

                print(f"🎉 총 {len(ipo_list)}개 IPO 정보 수집!")
                return ipo_list

            except Exception as e:
                print(f"❌ 크롤링 오류: {e}")
                return []

        def main():
            ipo_data = crawl_final_ipo_data()
            if not ipo_data:
                print("🚨 크롤링된 데이터가 없습니다.")
                return

            final_data = {
                'update_time': datetime.now().strftime('%Y년 %m월 %d일 %H시 %M분'),
                'data_source': '실시간 정보',
                'total_ipos': len(ipo_data),
                'ipo_list': ipo_data
            }
            
            output_file = 'n_ipo_data.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
            
            print(f"✅ '{output_file}' 파일 저장 완료!")

        if __name__ == "__main__":
            main()
        EOF
        
    - name: Commit and push results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Final IPO Crawler"
        
        if [ -f "n_ipo_data.json" ]; then
          git add n_ipo_data.json
          if git diff --staged --quiet; then
            echo "ℹ️ 변경사항 없음"
          else
            git commit -m "🚀 IPO 정보 최종 업데이트"
            git push
            echo "✅ 최종 IPO 데이터 푸시 완료"
          fi
        else
          echo "❌ 커밋할 파일이 없습니다"
        fi
