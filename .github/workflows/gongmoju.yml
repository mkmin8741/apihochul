name: IPO Auto Crawler

on:
  schedule:
    - cron: '0 */2 * * *'  # 2ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
  workflow_dispatch:

jobs:
  ipo-crawl:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 lxml pandas
        
    - name: IPO data crawling
      run: |
        python << 'EOF'
        import requests
        import json
        from datetime import datetime, timedelta
        import time
        import re
        from bs4 import BeautifulSoup

        def format_money_ipo(amount_str):
            """ê³µëª¨ê¸ˆì•¡ í¬ë§·íŒ…"""
            try:
                if not amount_str:
                    return "ë¯¸ì •"
                
                # ì–µì› ë‹¨ìœ„ ì²˜ë¦¬
                if 'ì–µ' in amount_str:
                    return amount_str
                
                # ìˆ«ìë§Œ ì¶”ì¶œí•´ì„œ ì–µì›ìœ¼ë¡œ ë³€í™˜
                numbers = re.findall(r'\d+', amount_str.replace(',', ''))
                if numbers:
                    amount = int(numbers[0])
                    if amount >= 100:
                        return f"{amount // 100}ì–µì›"
                    else:
                        return f"{amount}ì–µì›"
                
                return amount_str
            except:
                return "ë¯¸ì •"

        def crawl_38_communication():
            """38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì—ì„œ ê³µëª¨ì£¼ ì •ë³´ í¬ë¡¤ë§"""
            try:
                print("ğŸ¢ 38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í¬ë¡¤ë§ ì‹œì‘...")
                
                # ê³µëª¨ì²­ì•½ì¼ì • í˜ì´ì§€
                url = "http://www.38.co.kr/html/fund/?o=k"
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                }
                
                response = requests.get(url, headers=headers, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                
                ipo_list = []
                
                # í…Œì´ë¸”ì—ì„œ ê³µëª¨ì£¼ ì •ë³´ ì¶”ì¶œ
                table = soup.find('table', class_='tb_st01')
                if table:
                    rows = table.find_all('tr')[1:]  # í—¤ë” ì œì™¸
                    
                    for row in rows:
                        cols = row.find_all('td')
                        if len(cols) >= 6:
                            try:
                                company_name = cols[0].get_text().strip()
                                schedule = cols[1].get_text().strip()
                                confirmed_price = cols[2].get_text().strip()
                                hoped_price = cols[3].get_text().strip()
                                competition_ratio = cols[4].get_text().strip()
                                underwriter = cols[5].get_text().strip()
                                
                                if company_name and company_name != 'ì¢…ëª©ëª…':
                                    ipo_info = {
                                        'company_name': company_name,
                                        'schedule': schedule,
                                        'confirmed_price': confirmed_price if confirmed_price != '-' else 'ë¯¸ì •',
                                        'hoped_price': hoped_price,
                                        'competition_ratio': competition_ratio if competition_ratio != '-' else 'ë¯¸ì •',
                                        'underwriter': underwriter,
                                        'source': '38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜'
                                    }
                                    ipo_list.append(ipo_info)
                                    print(f"   âœ… {company_name} ì •ë³´ ìˆ˜ì§‘")
                                    
                            except Exception as e:
                                print(f"   âŒ í–‰ íŒŒì‹± ì˜¤ë¥˜: {e}")
                                continue
                
                print(f"âœ… 38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì—ì„œ {len(ipo_list)}ê°œ ê³µëª¨ì£¼ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
                return ipo_list
                
            except Exception as e:
                print(f"âŒ 38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                return []

        def crawl_additional_ipo_info():
            """ì¶”ê°€ ê³µëª¨ì£¼ ì •ë³´ í¬ë¡¤ë§ (ìˆ˜ìš”ì˜ˆì¸¡ ê²°ê³¼ ë“±)"""
            try:
                print("ğŸ“Š ì¶”ê°€ ê³µëª¨ì£¼ ì •ë³´ í¬ë¡¤ë§...")
                
                url = "http://www.38.co.kr/html/fund/?o=r1"
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
                }
                
                response = requests.get(url, headers=headers, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                
                additional_info = {}
                
                # ìˆ˜ìš”ì˜ˆì¸¡ ê²°ê³¼ í…Œì´ë¸” íŒŒì‹±
                table = soup.find('table', class_='tb_st01')
                if table:
                    rows = table.find_all('tr')[1:]
                    
                    for row in rows:
                        cols = row.find_all('td')
                        if len(cols) >= 4:
                            try:
                                company_name = cols[0].get_text().strip()
                                prediction_date = cols[1].get_text().strip()
                                price_range = cols[2].get_text().strip()
                                final_price = cols[3].get_text().strip()
                                
                                if company_name:
                                    additional_info[company_name] = {
                                        'prediction_date': prediction_date,
                                        'price_range': price_range,
                                        'final_price': final_price
                                    }
                                    
                            except Exception as e:
                                continue
                
                return additional_info
                
            except Exception as e:
                print(f"âŒ ì¶”ê°€ ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                return {}

        def get_current_ipo_status():
            """í˜„ì¬ ì§„í–‰ ì¤‘ì¸ ê³µëª¨ì£¼ ìƒíƒœ ë¶„ì„"""
            try:
                today = datetime.now()
                
                # ì˜¤ëŠ˜ ë‚ ì§œ ê¸°ì¤€ìœ¼ë¡œ ìƒíƒœ ë¶„ë¥˜
                status_info = {
                    'today_subscription': [],      # ì˜¤ëŠ˜ ì²­ì•½
                    'upcoming_subscription': [],   # ê³§ ì²­ì•½ ì˜ˆì •
                    'today_listing': [],          # ì˜¤ëŠ˜ ìƒì¥
                    'upcoming_listing': []        # ê³§ ìƒì¥ ì˜ˆì •
                }
                
                return status_info
                
            except Exception as e:
                print(f"âŒ ìƒíƒœ ë¶„ì„ ì‹¤íŒ¨: {e}")
                return {}

        def main():
            print('ğŸ¢ ê³µëª¨ì£¼ ì •ë³´ ìë™ í¬ë¡¤ë§ ì‹œì‘...')
            
            # ë©”ì¸ ê³µëª¨ì£¼ ì •ë³´ í¬ë¡¤ë§
            ipo_data = crawl_38_communication()
            time.sleep(2)
            
            # ì¶”ê°€ ì •ë³´ í¬ë¡¤ë§
            additional_info = crawl_additional_ipo_info()
            time.sleep(2)
            
            # í˜„ì¬ ìƒíƒœ ë¶„ì„
            status_info = get_current_ipo_status()
            
            # ë©”ì¸ ë°ì´í„°ì— ì¶”ê°€ ì •ë³´ ë³‘í•©
            for ipo in ipo_data:
                company_name = ipo['company_name']
                if company_name in additional_info:
                    ipo.update(additional_info[company_name])
            
            # ìµœì¢… JSON ìƒì„±
            final_data = {
                'updateTime': datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„'),
                'crawlTime': datetime.now().isoformat(),
                'totalIPOs': len(ipo_data),
                'dataSource': '38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ìë™í¬ë¡¤ë§',
                'status': status_info,
                'ipos': ipo_data
            }
            
            # JSON ì €ì¥
            with open('ipo_data.json', 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
                
            print(f'\nğŸ‰ ê³µëª¨ì£¼ í¬ë¡¤ë§ ì™„ë£Œ!')
            print(f'ğŸ“Š ì´ {len(ipo_data)}ê°œ ê³µëª¨ì£¼ ì •ë³´ ìˆ˜ì§‘')
            print(f'ğŸ• ì—…ë°ì´íŠ¸ ì‹œê°„: {final_data["updateTime"]}')
            
        if __name__ == "__main__":
            main()
        EOF
        
    - name: Commit and push
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "IPO Auto Crawler"
        git add ipo_data.json
        if git diff --staged --quiet; then
          echo "ë³€ê²½ì‚¬í•­ ì—†ìŒ"
        else
          git commit -m "ğŸ¢ ê³µëª¨ì£¼ ì •ë³´ ìë™ì—…ë°ì´íŠ¸ $(date +'%Y-%m-%d %H:%M')"
          git push
          echo "âœ… ê³µëª¨ì£¼ ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ"
        fi
