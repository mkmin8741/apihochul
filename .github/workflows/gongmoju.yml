name: n IPO Crawler

on:
  schedule:
    - cron: '0 */2 * * *'  # 2ì‹œê°„ë§ˆë‹¤ ì‹¤í–‰
  workflow_dispatch:

jobs:
  n-finance-crawler:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 lxml pandas
        
    - name: Crawl n Finance IPO Page
      run: |
        python << 'EOF'
        import requests
        import json
        from datetime import datetime
        import time
        from bs4 import BeautifulSoup
        from urllib.parse import urljoin

        def crawl_n_finance_ipo():
            """n ê¸ˆìœµ IPO íŽ˜ì´ì§€ë¥¼ ì •í™•í•˜ê²Œ í¬ë¡¤ë§"""
            try:
                print("ðŸ¢ n ê¸ˆìœµ IPO íŽ˜ì´ì§€ ì‹¤ì œ í¬ë¡¤ë§ ì‹œìž‘...")
                
                base_url = "https://finance.naver.com"
                url = urljoin(base_url, "/sise/ipo.naver")
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Referer': 'https://finance.naver.com/'
                }
                
                response = requests.get(url, headers=headers, timeout=20)
                response.encoding = 'euc-kr'
                
                if response.status_code != 200:
                    print(f"âŒ HTTP ì˜¤ë¥˜: {response.status_code}")
                    return []

                soup = BeautifulSoup(response.text, 'lxml')
                ipo_list = []
                
                # IPO ì •ë³´ê°€ ë‹´ê¸´ í…Œì´ë¸”ì˜ ëª¨ë“  í–‰(tr)ì„ ì°¾ìŒ
                rows = soup.select('table.type_7 tr')

                for row in rows:
                    item_area = row.find('div', class_='item_area')
                    if not item_area:
                        continue

                    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    company_name_tag = item_area.find('h4', class_='item_name')
                    company_name = company_name_tag.find('a').get_text(strip=True)
                    market_type = company_name_tag.find('span', class_='type').get_text(strip=True)
                    detail_page_url = urljoin(base_url, company_name_tag.find('a')['href'])
                    
                    info = {'company_name': company_name, 'market_type': market_type, 'detail_page_url': detail_page_url}

                    # ìƒì„¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸(ul)ì—ì„œ ë°ì´í„° ì¶”ì¶œ
                    info_list = item_area.find('ul', class_='lst_info')
                    if info_list:
                        items = info_list.find_all('li', recursive=False)
                        for item in items:
                            key = item.find('em', class_='tit').get_text(strip=True)
                            value_tag = item.find('span', class_='num')
                            value = value_tag.get_text(strip=True) if value_tag else item.get_text(strip=True).replace(key, '').strip()
                            
                            info_map = {
                                'ê³µëª¨ê°€': 'offer_price',
                                'ì—…ì¢…': 'industry',
                                'ì£¼ê´€ì‚¬': 'underwriters',
                                'ê°œì¸ì²­ì•½ê²½ìŸë¥ ': 'competition_rate',
                                'ì§„í–‰ìƒíƒœ': 'status',
                                'ê°œì¸ì²­ì•½': 'subscription_period',
                                'ìƒìž¥ì¼': 'listing_date'
                            }
                            if key in info_map:
                                info[info_map[key]] = value.replace('~', ' ~ ')
                    
                    # PDF ë§í¬ ì¶”ì¶œ
                    pdf_links_cell = row.find('td', class_=None)
                    if pdf_links_cell:
                        links = pdf_links_cell.find_all('a', class_='lst')
                        info['documents'] = []
                        for link in links:
                            link_text = link.get_text(strip=True)
                            link_href = link['href']
                            doc_type = 'ê¸°ì—…ê°œìš”' if 'ê¸°ì—…ê°œìš”' in link_text else 'IR BOOK' if 'IR BOOK' in link_text else 'ê¸°íƒ€'
                            info['documents'].append({'type': doc_type, 'url': link_href})

                    ipo_list.append(info)
                    print(f"   âœ… {market_type} '{company_name}' ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ (ìƒíƒœ: {info.get('status', 'N/A')})")

                print(f"ðŸŽ‰ ì´ {len(ipo_list)}ê°œ IPO ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ!")
                return ipo_list

            except Exception as e:
                print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                return []

        def main():
            ipo_data = crawl_n_finance_ipo()
            
            if not ipo_data:
                print("ðŸš¨ í¬ë¡¤ë§ëœ ë°ì´í„°ê°€ ì—†ì–´ íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return

            final_data = {
                'update_time': datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„'),
                'crawl_time': datetime.now().isoformat(),
                'data_source': 'n ê¸ˆìœµ IPO ì‹¤ì‹œê°„ í¬ë¡¤ë§',
                'source_url': 'https://finance.naver.com/sise/ipo.nhn',
                'total_ipos': len(ipo_data),
                'ipo_list': ipo_data
            }

            output_file = 'n_ipo_data.json'
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… '{output_file}' íŒŒì¼ ì €ìž¥ ì™„ë£Œ!")

        if __name__ == "__main__":
            main()
        EOF
        
    - name: Commit and push results
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "n IPO Crawler"
        
        if [ -f "n_ipo_data.json" ]; then
          git add n_ipo_data.json
          if git diff --staged --quiet; then
            echo "â„¹ï¸ ë³€ê²½ì‚¬í•­ ì—†ìŒ"
          else
            git commit -m "ðŸ¢ n ê¸ˆìœµ IPO ì‹¤ì‹œê°„ í¬ë¡¤ë§ $(date +'%Y-%m-%d %H:%M')"
            git push
            echo "âœ… n IPO ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ"
          fi
        else
          echo "âŒ ì»¤ë°‹í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤"
        fi
