name: Full Auto Crawler

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

jobs:
  full-auto:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 lxml
        
    - name: Full auto crawling system
      env:
        KOBIS_KEY: ${{ secrets.KOBIS_API_KEY }}
      run: |
        python << 'EOF'
        import requests
        import json
        from datetime import datetime, timedelta
        import os
        import time
        import re
        from bs4 import BeautifulSoup
        from urllib.parse import quote

        def format_date_korean(date_str):
            """ê°œë´‰ì¼ í•œêµ­ì–´ í¬ë§·íŒ…"""
            try:
                if not date_str:
                    return "ê°œë´‰ì¼ ë¯¸ì •"
                
                if '-' in date_str and len(date_str) == 10:
                    year, month, day = date_str.split('-')
                    return f"{year}ë…„ {month}ì›” {day}ì¼"
                
                if len(date_str) == 8 and date_str.isdigit():
                    year = date_str[:4]
                    month = date_str[4:6] 
                    day = date_str[6:8]
                    return f"{year}ë…„ {month}ì›” {day}ì¼"
                
                return date_str
            except:
                return "ê°œë´‰ì¼ ë¯¸ì •"

        def optimize_image_50kb(image_url):
            """ì´ë¯¸ì§€ 50KB ìµœì í™”"""
            try:
                if not image_url:
                    return ""
                
                if 'pstatic.net' in image_url:
                    # 50KB ë‚´ì™¸ ìµœì í™”
                    if '&size=' in image_url:
                        optimized = re.sub(r'&size=\d+x\d+', '&size=140x210', image_url)
                        if '&quality=' not in optimized:
                            optimized += '&quality=65'
                    else:
                        optimized = image_url + '&size=140x210&quality=65'
                    return optimized
                
                return image_url
            except:
                return image_url

        def crawl_naver_auto(movie_title):
            """ë„¤ì´ë²„ì—ì„œ ì™„ì „ ìë™ í¬ë¡¤ë§ - ì‚¬ìš©ì ì œê³µ êµ¬ì¡° í™œìš©"""
            try:
                print(f"ğŸ•·ï¸ '{movie_title}' ë„¤ì´ë²„ ìë™ í¬ë¡¤ë§...")
                
                # ë„¤ì´ë²„ ì˜í™” ê²€ìƒ‰ (ì‚¬ìš©ì ì œê³µ ë°©ì‹)
                search_url = "https://search.naver.com/search.naver"
                params = {
                    'where': 'nexearch',
                    'sm': 'tab_etc',
                    'mra': 'bkEw',
                    'pkid': '68',
                    'query': f'ì˜í™” {movie_title} ì •ë³´'
                }
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                }
                
                response = requests.get(search_url, params=params, headers=headers, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                
                result = {
                    'poster_url': '',
                    'plot': '',
                    'genre': '',
                    'runtime': '',
                    'grade': '',
                    'english_title': ''
                }
                
                # 1. í¬ìŠ¤í„° ì´ë¯¸ì§€ í¬ë¡¤ë§ (ì‚¬ìš©ì ì œê³µ êµ¬ì¡°)
                try:
                    # ì‚¬ìš©ì ì œê³µ: .thumb ._item img
                    thumb_link = soup.find('a', class_='thumb')
                    if not thumb_link:
                        # ëŒ€ì•ˆ ì…€ë ‰í„°
                        thumb_link = soup.select_one('a[class*="thumb"]')
                    
                    if thumb_link:
                        img_tag = thumb_link.find('img')
                        if img_tag and img_tag.get('src'):
                            poster_url = img_tag['src']
                            result['poster_url'] = optimize_image_50kb(poster_url)
                            print(f"   âœ… í¬ìŠ¤í„°: {result['poster_url'][:50]}...")
                    
                    # ì¶”ê°€ ì‹œë„
                    if not result['poster_url']:
                        img_tags = soup.find_all('img')
                        for img in img_tags:
                            src = img.get('src', '')
                            alt = img.get('alt', '')
                            if ('pstatic.net' in src and 'movie' in src) or movie_title in alt:
                                result['poster_url'] = optimize_image_50kb(src)
                                print(f"   âœ… ëŒ€ì•ˆ í¬ìŠ¤í„° ë°œê²¬")
                                break
                        
                except Exception as e:
                    print(f"   âŒ í¬ìŠ¤í„° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
                # 2. ì¤„ê±°ë¦¬ í¬ë¡¤ë§ (ì‚¬ìš©ì ì œê³µ êµ¬ì¡°)
                try:
                    # ì‚¬ìš©ì ì œê³µ: .intro_box._content .text._content_text
                    intro_box = soup.find('div', class_='intro_box')
                    if not intro_box:
                        intro_box = soup.select_one('div[class*="intro"]')
                    
                    if intro_box:
                        # ì‚¬ìš©ì ì œê³µ: .text._content_text
                        plot_element = intro_box.find('p', class_='text')
                        if not plot_element:
                            plot_element = intro_box.find('p')
                        
                        if plot_element:
                            plot_text = plot_element.get_text().strip()
                            if plot_text and len(plot_text) > 20:
                                # í…ìŠ¤íŠ¸ ì •ë¦¬
                                plot_clean = re.sub(r'\s+', ' ', plot_text).strip()
                                if not any(skip_word in plot_clean for skip_word in ['ë¡œê·¸ì¸', 'íšŒì›ê°€ì…', 'ë„¤ì´ë²„', 'ê²€ìƒ‰']):
                                    result['plot'] = plot_clean
                                    print(f"   âœ… ì¤„ê±°ë¦¬: {result['plot'][:50]}...")
                                
                except Exception as e:
                    print(f"   âŒ ì¤„ê±°ë¦¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
                # 3. ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (ì‚¬ìš©ì ì œê³µ êµ¬ì¡°)
                try:
                    # ì‚¬ìš©ì ì œê³µ: .info.txt_4 .info_group
                    info_area = soup.find('dl', class_='info')
                    if not info_area:
                        info_area = soup.select_one('dl[class*="info"]')
                    
                    if info_area:
                        # ì‚¬ìš©ì ì œê³µ: .info_group
                        info_groups = info_area.find_all('div', class_='info_group')
                        if not info_groups:
                            info_groups = info_area.find_all('div')
                        
                        for group in info_groups:
                            try:
                                dt = group.find('dt')
                                dd = group.find('dd')
                                if dt and dd:
                                    label_text = dt.get_text().strip()
                                    value_text = dd.get_text().strip()
                                    
                                    # ì •ë³´ ë§¤í•‘
                                    if 'ì¥ë¥´' in label_text and value_text:
                                        result['genre'] = value_text
                                        print(f"   âœ… ì¥ë¥´: {value_text}")
                                    elif 'ë“±ê¸‰' in label_text and value_text:
                                        result['grade'] = value_text
                                        print(f"   âœ… ë“±ê¸‰: {value_text}")
                                    elif 'ëŸ¬ë‹íƒ€ì„' in label_text and value_text:
                                        result['runtime'] = value_text
                                        print(f"   âœ… ëŸ¬ë‹íƒ€ì„: {value_text}")
                                        
                            except Exception as e:
                                print(f"   ìƒì„¸ì •ë³´ íŒŒì‹± ì˜¤ë¥˜: {e}")
                                continue
                                
                except Exception as e:
                    print(f"   âŒ ìƒì„¸ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
                # 4. ì˜ì–´ì œëª© í¬ë¡¤ë§
                try:
                    sub_title_area = soup.find('div', class_='sub_title')
                    if sub_title_area:
                        spans = sub_title_area.find_all('span', class_='txt')
                        for span in spans:
                            text = span.get_text().strip()
                            # ì˜ì–´ ì œëª© íŒ¨í„´ í™•ì¸
                            if re.match(r'^[A-Za-z\s:\'\.\-]+$', text) and len(text) > 2 and text not in ['ì˜í™”', '2025']:
                                result['english_title'] = text
                                print(f"   âœ… ì˜ì–´ì œëª©: {text}")
                                break
                                
                except Exception as e:
                    print(f"   âŒ ì˜ì–´ì œëª© í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
                # í¬ë¡¤ë§ ê²°ê³¼ ê²€ì¦
                success_count = sum(1 for v in result.values() if v)
                print(f"âœ… '{movie_title}' ìë™ í¬ë¡¤ë§ ì™„ë£Œ ({success_count}/6 í•­ëª© ì„±ê³µ)")
                
                return result
                
            except Exception as e:
                print(f"âŒ '{movie_title}' ì „ì²´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                return {
                    'poster_url': '',
                    'plot': 'ì¤„ê±°ë¦¬ ì •ë³´ë¥¼ ìˆ˜ì§‘ ì¤‘ì…ë‹ˆë‹¤.',
                    'genre': 'ì •ë³´ ìˆ˜ì§‘ ì¤‘',
                    'runtime': 'ì •ë³´ ìˆ˜ì§‘ ì¤‘',
                    'grade': 'ì •ë³´ ìˆ˜ì§‘ ì¤‘',
                    'english_title': ''
                }

        def main():
            print('ğŸ¤– ì™„ì „ ìë™í™” í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œì‘...')
            
            # KOBIS API í˜¸ì¶œ
            kobis_key = os.environ['KOBIS_KEY']
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
            
            print(f'ğŸ“… ì¡°íšŒ ë‚ ì§œ: {yesterday}')
            
            kobis_url = "http://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.json"
            kobis_params = {
                'key': kobis_key,
                'targetDt': yesterday,
                'itemPerPage': 10
            }
            
            kobis_response = requests.get(kobis_url, params=kobis_params, timeout=15)
            kobis_response.raise_for_status()
            kobis_data = kobis_response.json()
            
            movies = kobis_data['boxOfficeResult']['dailyBoxOfficeList']
            print(f'ğŸ“Š KOBISì—ì„œ {len(movies)}ê°œ ì˜í™” ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ')
            
            # ê° ì˜í™” ì™„ì „ ìë™ í¬ë¡¤ë§
            enhanced_movies = []
            
            for i, movie in enumerate(movies):
                print(f'\nğŸ¬ {i+1}/{len(movies)} - {movie["movieNm"]} ì™„ì „ ìë™ ì²˜ë¦¬...')
                
                movie_title = movie['movieNm']
                
                # ì™„ì „ ìë™ í¬ë¡¤ë§ (í•˜ë“œì½”ë”© ì—†ìŒ)
                movie_details = crawl_naver_auto(movie_title)
                time.sleep(3)  # ì•ˆì „í•œ í¬ë¡¤ë§ ê°„ê²©
                
                # ê°œë´‰ì¼ í¬ë§·íŒ…
                formatted_open_date = format_date_korean(movie.get('openDt', ''))
                
                # ìµœì¢… ë°ì´í„° ìƒì„±
                enhanced_movie = {
                    **movie,
                    **movie_details,
                    'formatted_open_date': formatted_open_date
                }
                
                enhanced_movies.append(enhanced_movie)
                print(f'âœ… {movie_title} ì™„ì „ ìë™ ì²˜ë¦¬ ì™„ë£Œ')
            
            # ìµœì¢… JSON ìƒì„±
            final_data = {
                'updateTime': datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„'),
                'targetDate': yesterday,
                'totalMovies': len(enhanced_movies),
                'dataSource': 'KOBIS + ë„¤ì´ë²„ ì™„ì „ìë™í¬ë¡¤ë§ (í•˜ë“œì½”ë”© ì—†ìŒ)',
                'imageOptimized': '50KB ìë™ìµœì í™”',
                'crawlingMethod': 'ì‚¬ìš©ìì œê³µ HTMLêµ¬ì¡° ê¸°ë°˜ ìë™í¬ë¡¤ë§',
                'movies': enhanced_movies
            }
            
            # JSON ì €ì¥
            with open('movies.json', 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
                
            print(f'\nğŸ‰ ì™„ì „ ìë™í™” í¬ë¡¤ë§ ì™„ë£Œ!')
            print(f'ğŸ“Š ì´ {len(enhanced_movies)}ê°œ ì˜í™”')
            print(f'ğŸ–¼ï¸ í¬ìŠ¤í„° ìë™ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("poster_url"))}ê°œ')
            print(f'ğŸ“ ì¤„ê±°ë¦¬ ìë™ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("plot") and "ìˆ˜ì§‘ ì¤‘" not in m["plot"])}ê°œ')
            print(f'ğŸ­ ìƒì„¸ì •ë³´ ìë™ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("genre") and "ìˆ˜ì§‘ ì¤‘" not in m["genre"])}ê°œ')
            
        if __name__ == "__main__":
            main()
        EOF
        
    - name: Commit and push
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Full Auto Crawler"
        git add movies.json
        if git diff --staged --quiet; then
          echo "ë³€ê²½ì‚¬í•­ ì—†ìŒ"
        else
          git commit -m "ğŸ¤– ì™„ì „ìë™í™”í¬ë¡¤ë§ - í•˜ë“œì½”ë”©ì—†ìŒ $(date +'%Y-%m-%d %H:%M')"
          git push
          echo "âœ… ì™„ì „ ìë™í™” ë°ì´í„° ì—…ë¡œë“œ"
        fi
