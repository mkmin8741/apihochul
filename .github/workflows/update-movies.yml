name: Movie Data Crawler

on:
  schedule:
    - cron: '0 */6 * * *'  # 6ì‹œê°„ë§ˆë‹¤
  workflow_dispatch:  # ìˆ˜ë™ ì‹¤í–‰ ê°€ëŠ¥

jobs:
  crawl-movie-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 lxml selenium webdriver-manager
        
    - name: Crawl complete movie data
      env:
        KOBIS_KEY: ${{ secrets.KOBIS_API_KEY }}
      run: |
        python << 'EOF'
        import requests
        import json
        from datetime import datetime, timedelta
        import os
        import time
        import re
        from bs4 import BeautifulSoup
        from urllib.parse import quote, urljoin
        import urllib.parse

        def clean_text(text):
            """í…ìŠ¤íŠ¸ ì •ë¦¬"""
            if not text:
                return ""
            return re.sub(r'\s+', ' ', text.strip())

        def get_naver_movie_details(movie_title, open_year=None):
            """ë„¤ì´ë²„ ì˜í™”ì—ì„œ ìƒì„¸ì •ë³´ í¬ë¡¤ë§"""
            try:
                print(f"ğŸ” ë„¤ì´ë²„ì—ì„œ '{movie_title}' ê²€ìƒ‰ ì¤‘...")
                
                # ë„¤ì´ë²„ ì˜í™” ê²€ìƒ‰
                search_query = f"{movie_title} ì˜í™”"
                search_url = "https://search.naver.com/search.naver"
                params = {
                    'sm': 'top_hty',
                    'fbm': '0',
                    'ie': 'utf8',
                    'query': search_query
                }
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                
                # ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€
                response = requests.get(search_url, params=params, headers=headers, timeout=10)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë„¤ì´ë²„ ì˜í™” ë§í¬ ì°¾ê¸°
                movie_link = None
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if 'movie.naver.com/movie/bi/mi/basic.nhn' in href or 'movie.naver.com/movie/bi/mi/basic.naver' in href:
                        movie_link = href
                        break
                
                if not movie_link:
                    print(f"âŒ '{movie_title}' ë„¤ì´ë²„ ì˜í™” í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return get_default_movie_data()
                
                print(f"âœ… ì˜í™” í˜ì´ì§€ ë°œê²¬: {movie_link}")
                
                # ì˜í™” ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
                time.sleep(1)  # ìš”ì²­ ê°„ê²©
                detail_response = requests.get(movie_link, headers=headers, timeout=10)
                detail_soup = BeautifulSoup(detail_response.text, 'html.parser')
                
                # í¬ìŠ¤í„° ì´ë¯¸ì§€
                poster_url = ""
                poster_img = detail_soup.find('div', class_='poster') 
                if poster_img:
                    img_tag = poster_img.find('img')
                    if img_tag and img_tag.get('src'):
                        poster_url = img_tag['src']
                        # https í”„ë¡œí† ì½œë¡œ ë³€ê²½
                        if poster_url.startswith('//'):
                            poster_url = 'https:' + poster_url
                        elif poster_url.startswith('http://'):
                            poster_url = poster_url.replace('http://', 'https://')
                
                # ì¤„ê±°ë¦¬
                plot = ""
                story_area = detail_soup.find('div', class_='story_area')
                if story_area:
                    story_p = story_area.find('p', class_='con_tx')
                    if story_p:
                        plot = clean_text(story_p.get_text())
                
                # ì˜í™” ì •ë³´
                info_area = detail_soup.find('dl', class_='info_spec')
                genre = ""
                director = ""
                runtime = ""
                grade = ""
                
                if info_area:
                    dts = info_area.find_all('dt')
                    dds = info_area.find_all('dd')
                    
                    for i, dt in enumerate(dts):
                        if i < len(dds):
                            dt_text = clean_text(dt.get_text())
                            dd_text = clean_text(dds[i].get_text())
                            
                            if 'ì¥ë¥´' in dt_text:
                                genre = dd_text
                            elif 'ê°ë…' in dt_text:
                                director = dd_text
                            elif 'ë“±ê¸‰' in dt_text or 'ê´€ëŒê°€' in dt_text:
                                grade = dd_text
                
                # ëŸ¬ë‹íƒ€ì„ ì°¾ê¸°
                runtime_span = detail_soup.find('span', class_='running_time')
                if runtime_span:
                    runtime = clean_text(runtime_span.get_text())
                
                # ì˜ì–´ì œëª© ì°¾ê¸°
                english_title = ""
                title_area = detail_soup.find('div', class_='mv_info_area')
                if title_area:
                    h3_tag = title_area.find('h3', class_='h_movie')
                    if h3_tag:
                        strong_tag = h3_tag.find('strong')
                        if strong_tag:
                            # í•œêµ­ì–´ ì œëª© ë‹¤ìŒì˜ ì˜ì–´ì œëª© ì°¾ê¸°
                            next_text = strong_tag.next_sibling
                            if next_text and isinstance(next_text, str):
                                english_match = re.search(r'([A-Za-z\s:\']+)', next_text.strip())
                                if english_match:
                                    english_title = english_match.group(1).strip()
                
                result = {
                    'poster_url': poster_url,
                    'plot': plot or "ì¤„ê±°ë¦¬ ì •ë³´ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.",
                    'genre': genre or "ì •ë³´ ì¤€ë¹„ ì¤‘",
                    'director': director or "ì •ë³´ ì¤€ë¹„ ì¤‘", 
                    'runtime': runtime or "ì •ë³´ ì¤€ë¹„ ì¤‘",
                    'grade': grade or "ì •ë³´ ì¤€ë¹„ ì¤‘",
                    'english_title': english_title
                }
                
                print(f"âœ… '{movie_title}' ë„¤ì´ë²„ í¬ë¡¤ë§ ì™„ë£Œ")
                print(f"   í¬ìŠ¤í„°: {'ìˆìŒ' if poster_url else 'ì—†ìŒ'}")
                print(f"   ì¤„ê±°ë¦¬: {'ìˆìŒ' if plot else 'ì—†ìŒ'}")
                print(f"   ì¥ë¥´: {genre}")
                
                return result
                
            except Exception as e:
                print(f"âŒ '{movie_title}' ë„¤ì´ë²„ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                return get_default_movie_data()

        def get_daum_movie_details(movie_title, open_year=None):
            """ë‹¤ìŒ ì˜í™”ì—ì„œ ìƒì„¸ì •ë³´ í¬ë¡¤ë§"""
            try:
                print(f"ğŸ” ë‹¤ìŒì—ì„œ '{movie_title}' ê²€ìƒ‰ ì¤‘...")
                
                # ë‹¤ìŒ ì˜í™” ê²€ìƒ‰
                search_query = f"{movie_title} ì˜í™”"
                search_url = "https://search.daum.net/search"
                params = {
                    'w': 'tot',
                    'q': search_query
                }
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
                
                response = requests.get(search_url, params=params, headers=headers, timeout=10)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # ë‹¤ìŒ ì˜í™” ë§í¬ ì°¾ê¸°
                movie_link = None
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    if 'movie.daum.net/moviedb/main' in href:
                        movie_link = href
                        break
                
                if not movie_link:
                    print(f"âŒ '{movie_title}' ë‹¤ìŒ ì˜í™” í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return get_default_movie_data()
                
                print(f"âœ… ë‹¤ìŒ ì˜í™” í˜ì´ì§€ ë°œê²¬: {movie_link}")
                
                # ì˜í™” ìƒì„¸ í˜ì´ì§€ í¬ë¡¤ë§
                time.sleep(1)
                detail_response = requests.get(movie_link, headers=headers, timeout=10)
                detail_soup = BeautifulSoup(detail_response.text, 'html.parser')
                
                # í¬ìŠ¤í„°
                poster_url = ""
                poster_div = detail_soup.find('div', class_='poster_movie')
                if poster_div:
                    img_tag = poster_div.find('img')
                    if img_tag and img_tag.get('src'):
                        poster_url = img_tag['src']
                        if poster_url.startswith('//'):
                            poster_url = 'https:' + poster_url
                
                # ì¤„ê±°ë¦¬
                plot = ""
                story_div = detail_soup.find('div', class_='desc_movie')
                if story_div:
                    plot_p = story_div.find('p')
                    if plot_p:
                        plot = clean_text(plot_p.get_text())
                
                # ìƒì„¸ ì •ë³´
                info_movie = detail_soup.find('div', class_='info_movie')
                genre = ""
                director = ""
                runtime = ""
                grade = ""
                english_title = ""
                
                if info_movie:
                    # ê°ì¢… ì •ë³´ íŒŒì‹±
                    dls = info_movie.find_all('dl')
                    for dl in dls:
                        dt = dl.find('dt')
                        dd = dl.find('dd')
                        if dt and dd:
                            dt_text = clean_text(dt.get_text())
                            dd_text = clean_text(dd.get_text())
                            
                            if 'ì¥ë¥´' in dt_text:
                                genre = dd_text
                            elif 'ê°ë…' in dt_text:
                                director = dd_text
                            elif 'ëŸ¬ë‹íƒ€ì„' in dt_text:
                                runtime = dd_text
                            elif 'ë“±ê¸‰' in dt_text or 'ê´€ëŒê°€' in dt_text:
                                grade = dd_text
                
                # ì˜ì–´ì œëª© ì°¾ê¸°
                title_en = detail_soup.find('span', class_='txt_title_en')
                if title_en:
                    english_title = clean_text(title_en.get_text())
                
                result = {
                    'poster_url': poster_url,
                    'plot': plot or "ì¤„ê±°ë¦¬ ì •ë³´ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.",
                    'genre': genre or "ì •ë³´ ì¤€ë¹„ ì¤‘",
                    'director': director or "ì •ë³´ ì¤€ë¹„ ì¤‘",
                    'runtime': runtime or "ì •ë³´ ì¤€ë¹„ ì¤‘", 
                    'grade': grade or "ì •ë³´ ì¤€ë¹„ ì¤‘",
                    'english_title': english_title
                }
                
                print(f"âœ… '{movie_title}' ë‹¤ìŒ í¬ë¡¤ë§ ì™„ë£Œ")
                return result
                
            except Exception as e:
                print(f"âŒ '{movie_title}' ë‹¤ìŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                return get_default_movie_data()

        def get_default_movie_data():
            """ê¸°ë³¸ ì˜í™” ë°ì´í„°"""
            return {
                'poster_url': "",
                'plot': "ì¤„ê±°ë¦¬ ì •ë³´ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.",
                'genre': "ì •ë³´ ì¤€ë¹„ ì¤‘",
                'director': "ì •ë³´ ì¤€ë¹„ ì¤‘",
                'runtime': "ì •ë³´ ì¤€ë¹„ ì¤‘",
                'grade': "ì •ë³´ ì¤€ë¹„ ì¤‘",
                'english_title': ""
            }

        def main():
            print('ğŸ¬ ì™„ì „ í¬ë¡¤ë§ ìë™í™” ì‹œì‘...')
            
            # KOBIS API í˜¸ì¶œ
            kobis_key = os.environ['KOBIS_KEY']
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
            
            print(f'ğŸ“… ì¡°íšŒ ë‚ ì§œ: {yesterday}')
            
            kobis_url = "http://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.json"
            kobis_params = {
                'key': kobis_key,
                'targetDt': yesterday,
                'itemPerPage': 10
            }
            
            kobis_response = requests.get(kobis_url, params=kobis_params, timeout=15)
            kobis_response.raise_for_status()
            kobis_data = kobis_response.json()
            
            movies = kobis_data['boxOfficeResult']['dailyBoxOfficeList']
            print(f'ğŸ“Š KOBISì—ì„œ {len(movies)}ê°œ ì˜í™” ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ')
            
            # ê° ì˜í™”ì— ëŒ€í•´ í¬ë¡¤ë§ìœ¼ë¡œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
            enhanced_movies = []
            
            for i, movie in enumerate(movies):
                print(f'\nğŸ•·ï¸ {i+1}/{len(movies)} - {movie["movieNm"]} í¬ë¡¤ë§ ì¤‘...')
                
                movie_title = movie['movieNm']
                open_year = None
                
                # ê°œë´‰ë…„ë„ ì¶”ì¶œ
                if movie.get('openDt'):
                    try:
                        open_year = int(movie['openDt'][:4])
                    except:
                        pass
                
                # ë„¤ì´ë²„ì—ì„œ ë¨¼ì € ì‹œë„
                movie_details = get_naver_movie_details(movie_title, open_year)
                time.sleep(2)  # í¬ë¡¤ë§ ê°„ê²©
                
                # ë„¤ì´ë²„ì—ì„œ ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ ë‹¤ìŒì—ì„œë„ ì‹œë„
                if (not movie_details['poster_url'] or 
                    movie_details['plot'] == "ì¤„ê±°ë¦¬ ì •ë³´ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤." or
                    movie_details['genre'] == "ì •ë³´ ì¤€ë¹„ ì¤‘"):
                    
                    print(f"ğŸ”„ '{movie_title}' ë‹¤ìŒì—ì„œë„ í¬ë¡¤ë§ ì‹œë„...")
                    daum_details = get_daum_movie_details(movie_title, open_year)
                    
                    # ë” ì¢‹ì€ ì •ë³´ê°€ ìˆìœ¼ë©´ êµì²´
                    if daum_details['poster_url'] and not movie_details['poster_url']:
                        movie_details['poster_url'] = daum_details['poster_url']
                    if daum_details['plot'] != "ì¤„ê±°ë¦¬ ì •ë³´ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤." and movie_details['plot'] == "ì¤„ê±°ë¦¬ ì •ë³´ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.":
                        movie_details['plot'] = daum_details['plot'] 
                    if daum_details['genre'] != "ì •ë³´ ì¤€ë¹„ ì¤‘" and movie_details['genre'] == "ì •ë³´ ì¤€ë¹„ ì¤‘":
                        movie_details['genre'] = daum_details['genre']
                    if daum_details['director'] != "ì •ë³´ ì¤€ë¹„ ì¤‘" and movie_details['director'] == "ì •ë³´ ì¤€ë¹„ ì¤‘":
                        movie_details['director'] = daum_details['director']
                    
                    time.sleep(2)  # í¬ë¡¤ë§ ê°„ê²©
                
                # ê¸°ì¡´ KOBIS ë°ì´í„°ì— í¬ë¡¤ë§ ì •ë³´ ì¶”ê°€
                enhanced_movie = {
                    **movie,  # ê¸°ì¡´ KOBIS ë°ì´í„° ìœ ì§€
                    **movie_details  # í¬ë¡¤ë§í•œ ìƒì„¸ ì •ë³´ ì¶”ê°€
                }
                
                enhanced_movies.append(enhanced_movie)
                print(f'âœ… {movie_title} ì™„ë£Œ - í¬ìŠ¤í„°: {"ìˆìŒ" if movie_details["poster_url"] else "ì—†ìŒ"}')
            
            # ìµœì¢… ë°ì´í„° ìƒì„±
            final_data = {
                'updateTime': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'targetDate': yesterday,
                'totalMovies': len(enhanced_movies),
                'dataSource': 'KOBIS + ë„¤ì´ë²„ì˜í™” + ë‹¤ìŒì˜í™” í¬ë¡¤ë§',
                'movies': enhanced_movies
            }
            
            # JSON íŒŒì¼ ì €ì¥
            with open('movies.json', 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
                
            print(f'\nğŸ‰ ì™„ì „ í¬ë¡¤ë§ ìë™í™” ì™„ë£Œ!')
            print(f'ğŸ“Š ì´ {len(enhanced_movies)}ê°œ ì˜í™”')
            print(f'ğŸ–¼ï¸ í¬ìŠ¤í„° ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("poster_url"))}ê°œ')
            print(f'ğŸ“ ì¤„ê±°ë¦¬ ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("plot") and "ì¤€ë¹„ ì¤‘" not in m["plot"])}ê°œ')
            print(f'ğŸ­ ì¥ë¥´ ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("genre") and "ì¤€ë¹„ ì¤‘" not in m["genre"])}ê°œ')
            
        if __name__ == "__main__":
            main()
        EOF
        
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Movie Crawler Bot"
        git add movies.json
        if git diff --staged --quiet; then
          echo "ë³€ê²½ì‚¬í•­ ì—†ìŒ"
        else
          git commit -m "ğŸ•·ï¸ ì˜í™” ì •ë³´ ìë™ í¬ë¡¤ë§ ì—…ë°ì´íŠ¸ $(date +'%Y-%m-%d %H:%M')"
          git push
          echo "âœ… í¬ë¡¤ë§ ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ"
        fi
