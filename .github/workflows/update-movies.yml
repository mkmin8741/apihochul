name: Auto Crawling System

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

jobs:
  auto-crawl:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 lxml selenium webdriver-manager
        
    - name: Auto crawling all movie data
      env:
        KOBIS_KEY: ${{ secrets.KOBIS_API_KEY }}
      run: |
        python << 'EOF'
        import requests
        import json
        from datetime import datetime, timedelta
        import os
        import time
        import re
        from bs4 import BeautifulSoup
        from urllib.parse import quote

        def format_date_korean(date_str):
            """ê°œë´‰ì¼ í•œêµ­ì–´ í¬ë§·íŒ…"""
            try:
                if not date_str:
                    return "ê°œë´‰ì¼ ë¯¸ì •"
                
                if '-' in date_str and len(date_str) == 10:
                    year, month, day = date_str.split('-')
                    return f"{year}ë…„ {month}ì›” {day}ì¼"
                
                if len(date_str) == 8 and date_str.isdigit():
                    year = date_str[:4]
                    month = date_str[4:6] 
                    day = date_str[6:8]
                    return f"{year}ë…„ {month}ì›” {day}ì¼"
                
                return date_str
            except:
                return "ê°œë´‰ì¼ ë¯¸ì •"

        def optimize_image_50kb(image_url):
            """ì´ë¯¸ì§€ 50KB ìµœì í™”"""
            try:
                if not image_url:
                    return ""
                
                if 'pstatic.net' in image_url:
                    # 50KB ë‚´ì™¸ê°€ ë˜ë„ë¡ í¬ê¸°ì™€ í’ˆì§ˆ ì¡°ì •
                    optimized = image_url.replace('&size=176x264', '&size=140x210&quality=65')
                    return optimized
                
                return image_url
            except:
                return image_url

        def crawl_naver_movie_auto(movie_title):
            """ë„¤ì´ë²„ì—ì„œ ì™„ì „ ìžë™ í¬ë¡¤ë§"""
            try:
                print(f"ðŸ” '{movie_title}' ìžë™ í¬ë¡¤ë§ ì‹œìž‘...")
                
                # ë„¤ì´ë²„ ì˜í™” ê²€ìƒ‰
                search_url = "https://search.naver.com/search.naver"
                params = {
                    'where': 'nexearch',
                    'sm': 'tab_etc', 
                    'query': f'ì˜í™” {movie_title} ì •ë³´'
                }
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive'
                }
                
                response = requests.get(search_url, params=params, headers=headers, timeout=15)
                soup = BeautifulSoup(response.text, 'html.parser')
                
                result = {
                    'poster_url': '',
                    'plot': '',
                    'genre': '',
                    'director': '',
                    'runtime': '',
                    'grade': '',
                    'english_title': ''
                }
                
                # 1. í¬ìŠ¤í„° ì´ë¯¸ì§€ í¬ë¡¤ë§
                try:
                    # ì‚¬ìš©ìžê°€ ë³´ì—¬ì¤€ ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜
                    poster_link = soup.find('a', class_='thumb')
                    if not poster_link:
                        poster_link = soup.find('a', {'class': lambda x: x and 'thumb' in x})
                    
                    if poster_link:
                        img_tag = poster_link.find('img')
                        if img_tag and img_tag.get('src'):
                            poster_url = img_tag['src']
                            result['poster_url'] = optimize_image_50kb(poster_url)
                            print(f"   âœ… í¬ìŠ¤í„° ë°œê²¬: {result['poster_url'][:50]}...")
                    
                    # ëŒ€ì•ˆ ë°©ë²•
                    if not result['poster_url']:
                        for img in soup.find_all('img'):
                            src = img.get('src', '')
                            alt = img.get('alt', '')
                            if 'pstatic.net' in src and (movie_title in alt or 'movie' in src):
                                result['poster_url'] = optimize_image_50kb(src)
                                print(f"   âœ… ëŒ€ì•ˆ í¬ìŠ¤í„° ë°œê²¬")
                                break
                except Exception as e:
                    print(f"   âŒ í¬ìŠ¤í„° í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
                # 2. ì¤„ê±°ë¦¬ í¬ë¡¤ë§ (ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜)
                try:
                    # ì‚¬ìš©ìž ì œê³µ êµ¬ì¡°: .intro_box._content .text._content_text
                    intro_box = soup.find('div', class_='intro_box')
                    if not intro_box:
                        intro_box = soup.find('div', {'class': lambda x: x and 'intro' in x})
                    
                    if intro_box:
                        plot_element = intro_box.find('p', class_='text')
                        if not plot_element:
                            plot_element = intro_box.find('p')
                        
                        if plot_element:
                            plot_text = plot_element.get_text().strip()
                            if plot_text and len(plot_text) > 20:
                                result['plot'] = re.sub(r'\s+', ' ', plot_text)
                                print(f"   âœ… ì¤„ê±°ë¦¬ ë°œê²¬: {result['plot'][:50]}...")
                    
                    # ëŒ€ì•ˆ ë°©ë²• - íŽ˜ì´ì§€ ì „ì²´ì—ì„œ ì¤„ê±°ë¦¬ íŒ¨í„´ ì°¾ê¸°
                    if not result['plot']:
                        page_text = soup.get_text()
                        sentences = re.split(r'[.!?]\s*', page_text)
                        for sentence in sentences:
                            if (len(sentence) > 50 and len(sentence) < 500 and
                                (movie_title in sentence or 'ì˜í™”' in sentence) and
                                ('ì¤„ê±°ë¦¬' in sentence or 'ì†Œê°œ' in sentence or 'ì´ì•¼ê¸°' in sentence)):
                                result['plot'] = sentence.strip()
                                print(f"   âœ… ëŒ€ì•ˆ ì¤„ê±°ë¦¬ ë°œê²¬")
                                break
                except Exception as e:
                    print(f"   âŒ ì¤„ê±°ë¦¬ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
                # 3. ìƒì„¸ ì •ë³´ í¬ë¡¤ë§ (ì‹¤ì œ êµ¬ì¡° ê¸°ë°˜)
                try:
                    # ì‚¬ìš©ìž ì œê³µ êµ¬ì¡°: .info.txt_4 .info_group
                    info_area = soup.find('dl', class_='info')
                    if not info_area:
                        info_area = soup.find('dl', {'class': lambda x: x and 'info' in x})
                    
                    if info_area:
                        info_groups = info_area.find_all('div', class_='info_group')
                        if not info_groups:
                            info_groups = info_area.find_all('div')
                        
                        for group in info_groups:
                            try:
                                dt = group.find('dt')
                                dd = group.find('dd')
                                if dt and dd:
                                    label = dt.get_text().strip().replace('|', '').strip()
                                    value = dd.get_text().strip()
                                    
                                    if 'ìž¥ë¥´' in label and value:
                                        result['genre'] = value
                                        print(f"   âœ… ìž¥ë¥´: {value}")
                                    elif 'ë“±ê¸‰' in label and value:
                                        result['grade'] = value  
                                        print(f"   âœ… ë“±ê¸‰: {value}")
                                    elif 'ëŸ¬ë‹íƒ€ìž„' in label and value:
                                        result['runtime'] = value
                                        print(f"   âœ… ëŸ¬ë‹íƒ€ìž„: {value}")
                            except:
                                continue
                    
                    # ëŒ€ì•ˆ ë°©ë²• - í…ìŠ¤íŠ¸ íŒ¨í„´ ë§¤ì¹­
                    if not any([result['genre'], result['grade'], result['runtime']]):
                        page_text = soup.get_text()
                        
                        # ìž¥ë¥´ íŒ¨í„´
                        genre_match = re.search(r'ìž¥ë¥´[:\s]*([ê°€-íž£,\s]+(?:ì½”ë¯¸ë””|ë“œë¼ë§ˆ|ì•¡ì…˜|ìŠ¤ë¦´ëŸ¬|ê³µí¬|ë¡œë§¨ìŠ¤|íŒíƒ€ì§€|SF|ì• ë‹ˆë©”ì´ì…˜)[ê°€-íž£,\s]*)', page_text)
                        if genre_match:
                            result['genre'] = genre_match.group(1).strip()
                        
                        # ë“±ê¸‰ íŒ¨í„´
                        grade_match = re.search(r'(ì „ì²´ê´€ëžŒê°€|12ì„¸ì´ìƒê´€ëžŒê°€|15ì„¸ì´ìƒê´€ëžŒê°€|ì²­ì†Œë…„ê´€ëžŒë¶ˆê°€)', page_text)
                        if grade_match:
                            result['grade'] = grade_match.group(1)
                        
                        # ëŸ¬ë‹íƒ€ìž„ íŒ¨í„´
                        runtime_match = re.search(r'(\d+ë¶„)', page_text)
                        if runtime_match:
                            result['runtime'] = runtime_match.group(1)
                
                except Exception as e:
                    print(f"   âŒ ìƒì„¸ì •ë³´ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
                # 4. ì˜ì–´ì œëª© í¬ë¡¤ë§
                try:
                    sub_title = soup.find('div', class_='sub_title')
                    if sub_title:
                        spans = sub_title.find_all('span', class_='txt')
                        for span in spans:
                            text = span.get_text().strip()
                            if re.match(r'^[A-Za-z\s:\'\.]+$', text) and text not in ['ì˜í™”', '2025']:
                                result['english_title'] = text
                                print(f"   âœ… ì˜ì–´ì œëª©: {text}")
                                break
                except Exception as e:
                    print(f"   âŒ ì˜ì–´ì œëª© í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
                # 5. ê°ë… ì •ë³´ ë³„ë„ ê²€ìƒ‰
                try:
                    director_search_url = "https://search.naver.com/search.naver"
                    director_params = {
                        'query': f'{movie_title} ê°ë…',
                        'where': 'nexearch'
                    }
                    
                    director_response = requests.get(director_search_url, params=director_params, headers=headers, timeout=10)
                    director_soup = BeautifulSoup(director_response.text, 'html.parser')
                    director_text = director_soup.get_text()
                    
                    director_match = re.search(r'ê°ë…[:\s]*([ê°€-íž£]{2,4})', director_text)
                    if director_match:
                        result['director'] = director_match.group(1)
                        print(f"   âœ… ê°ë…: {result['director']}")
                    
                except Exception as e:
                    print(f"   âŒ ê°ë… í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                
                # ê¸°ë³¸ê°’ ì„¤ì •
                for key, value in result.items():
                    if not value:
                        if key == 'plot':
                            result[key] = 'ì¤„ê±°ë¦¬ ì •ë³´ë¥¼ ìˆ˜ì§‘ ì¤‘ìž…ë‹ˆë‹¤.'
                        else:
                            result[key] = 'ì •ë³´ ìˆ˜ì§‘ ì¤‘'
                
                print(f"âœ… '{movie_title}' ìžë™ í¬ë¡¤ë§ ì™„ë£Œ")
                return result
                
            except Exception as e:
                print(f"âŒ '{movie_title}' ìžë™ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                return {
                    'poster_url': '',
                    'plot': 'ì¤„ê±°ë¦¬ ì •ë³´ë¥¼ ìˆ˜ì§‘ ì¤‘ìž…ë‹ˆë‹¤.',
                    'genre': 'ì •ë³´ ìˆ˜ì§‘ ì¤‘',
                    'director': 'ì •ë³´ ìˆ˜ì§‘ ì¤‘', 
                    'runtime': 'ì •ë³´ ìˆ˜ì§‘ ì¤‘',
                    'grade': 'ì •ë³´ ìˆ˜ì§‘ ì¤‘',
                    'english_title': ''
                }

        def main():
            print('ðŸ•·ï¸ ì™„ì „ ìžë™ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì‹œìž‘...')
            
            # KOBIS API í˜¸ì¶œ
            kobis_key = os.environ['KOBIS_KEY']
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
            
            print(f'ðŸ“… ì¡°íšŒ ë‚ ì§œ: {yesterday}')
            
            kobis_url = "http://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.json"
            kobis_params = {
                'key': kobis_key,
                'targetDt': yesterday,
                'itemPerPage': 10
            }
            
            kobis_response = requests.get(kobis_url, params=kobis_params, timeout=15)
            kobis_response.raise_for_status()
            kobis_data = kobis_response.json()
            
            movies = kobis_data['boxOfficeResult']['dailyBoxOfficeList']
            print(f'ðŸ“Š KOBISì—ì„œ {len(movies)}ê°œ ì˜í™” ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ')
            
            # ê° ì˜í™” ì™„ì „ ìžë™ í¬ë¡¤ë§
            enhanced_movies = []
            
            for i, movie in enumerate(movies):
                print(f'\nðŸ•·ï¸ {i+1}/{len(movies)} - {movie["movieNm"]} ì™„ì „ ìžë™ í¬ë¡¤ë§...')
                
                movie_title = movie['movieNm']
                
                # ì™„ì „ ìžë™ í¬ë¡¤ë§
                movie_details = crawl_naver_movie_auto(movie_title)
                time.sleep(3)  # ì•ˆì „í•œ í¬ë¡¤ë§ ê°„ê²©
                
                # ê°œë´‰ì¼ í¬ë§·íŒ…
                formatted_open_date = format_date_korean(movie.get('openDt', ''))
                
                # ìµœì¢… ë°ì´í„° ìƒì„±
                enhanced_movie = {
                    **movie,
                    **movie_details,
                    'formatted_open_date': formatted_open_date
                }
                
                enhanced_movies.append(enhanced_movie)
                print(f'âœ… {movie_title} ì™„ì „ ìžë™ ì²˜ë¦¬ ì™„ë£Œ')
            
            # ìµœì¢… JSON ìƒì„±
            final_data = {
                'updateTime': datetime.now().strftime('%Yë…„ %mì›” %dì¼ %Hì‹œ %Më¶„'),
                'targetDate': yesterday,
                'totalMovies': len(enhanced_movies),
                'dataSource': 'KOBIS + ì™„ì „ìžë™í¬ë¡¤ë§ (í•˜ë“œì½”ë”©ì—†ìŒ)',
                'imageOptimized': '50KB ìžë™ìµœì í™”',
                'crawlingMethod': 'ì‹¤ì‹œê°„ ìžë™ ë°ì´í„° ìˆ˜ì§‘',
                'movies': enhanced_movies
            }
            
            # JSON ì €ìž¥
            with open('movies.json', 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
                
            print(f'\nðŸŽ‰ ì™„ì „ ìžë™ í¬ë¡¤ë§ ì™„ë£Œ!')
            print(f'ðŸ“Š ì´ {len(enhanced_movies)}ê°œ ì˜í™”')
            print(f'ðŸ–¼ï¸ í¬ìŠ¤í„° ìžë™ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("poster_url"))}ê°œ')
            print(f'ðŸ“ ì¤„ê±°ë¦¬ ìžë™ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("plot") and "ìˆ˜ì§‘ ì¤‘" not in m["plot"])}ê°œ')
            print(f'ðŸŽ­ ìƒì„¸ì •ë³´ ìžë™ìˆ˜ì§‘: {sum(1 for m in enhanced_movies if m.get("genre") and "ìˆ˜ì§‘ ì¤‘" not in m["genre"])}ê°œ')
            
        if __name__ == "__main__":
            main()
        EOF
        
    - name: Commit and push
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Auto Crawler Bot"
        git add movies.json
        if git diff --staged --quiet; then
          echo "ë³€ê²½ì‚¬í•­ ì—†ìŒ"
        else
          git commit -m "ðŸ¤– ì™„ì „ìžë™í¬ë¡¤ë§ - í•˜ë“œì½”ë”©ì—†ìŒ $(date +'%Y-%m-%d %H:%M')"
          git push
          echo "âœ… ì™„ì „ ìžë™ í¬ë¡¤ë§ ë°ì´í„° ì—…ë¡œë“œ"
        fi
