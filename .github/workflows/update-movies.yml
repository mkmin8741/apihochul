name: Full Auto Crawler

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

jobs:
  full-auto:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        pip install requests beautifulsoup4 lxml
        
    - name: Full auto crawling system
      env:
        KOBIS_KEY: ${{ secrets.KOBIS_API_KEY }}
      run: |
        python << 'EOF'
        import requests
        import json
        from datetime import datetime, timedelta
        import os
        import time
        import re
        from bs4 import BeautifulSoup
        from urllib.parse import quote

        def format_date_korean(date_str):
            """Í∞úÎ¥âÏùº ÌïúÍµ≠Ïñ¥ Ìè¨Îß∑ÌåÖ"""
            try:
                if not date_str:
                    return "Í∞úÎ¥âÏùº ÎØ∏Ï†ï"
                
                if '-' in date_str and len(date_str) == 10:
                    year, month, day = date_str.split('-')
                    return f"{year}ÎÖÑ {month}Ïõî {day}Ïùº"
                
                if len(date_str) == 8 and date_str.isdigit():
                    year = date_str[:4]
                    month = date_str[4:6] 
                    day = date_str[6:8]
                    return f"{year}ÎÖÑ {month}Ïõî {day}Ïùº"
                
                return date_str
            except:
                return "Í∞úÎ¥âÏùº ÎØ∏Ï†ï"

        def optimize_image_50kb(image_url):
            """Ïù¥ÎØ∏ÏßÄ 50KB ÏµúÏ†ÅÌôî"""
            try:
                if not image_url:
                    return ""
                
                if 'pstatic.net' in image_url:
                    # 50KB ÎÇ¥Ïô∏ ÏµúÏ†ÅÌôî
                    if '&size=' in image_url:
                        optimized = re.sub(r'&size=\d+x\d+', '&size=140x210', image_url)
                        if '&quality=' not in optimized:
                            optimized += '&quality=65'
                    else:
                        optimized = image_url + '&size=140x210&quality=65'
                    return optimized
                
                return image_url
            except:
                return image_url

        def crawl_naver_auto(movie_title):
            """ÎÑ§Ïù¥Î≤ÑÏóêÏÑú ÏôÑÏ†Ñ ÏûêÎèô ÌÅ¨Î°§ÎßÅ - ÏÇ¨Ïö©Ïûê Ï†úÍ≥µ Íµ¨Ï°∞ ÌôúÏö©"""
            try:
                print(f"üï∑Ô∏è '{movie_title}' ÎÑ§Ïù¥Î≤Ñ ÏûêÎèô ÌÅ¨Î°§ÎßÅ...")
                
                # ÎÑ§Ïù¥Î≤Ñ ÏòÅÌôî Í≤ÄÏÉâ (ÏÇ¨Ïö©Ïûê Ï†úÍ≥µ Î∞©Ïãù)
                search_url = "https://search.naver.com/search.naver"
                params = {
                    'where': 'nexearch',
                    'sm': 'tab_etc',
                    'mra': 'bkEw',
                    'pkid': '68',
                    'query': f'ÏòÅÌôî {movie_title} Ï†ïÎ≥¥'
                }
                
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                }
                
                response = requests.get(search_url, params=params, headers=headers, timeout=15)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                
                result = {
                    'poster_url': '',
                    'plot': '',
                    'genre': '',
                    'runtime': '',
                    'grade': '',
                    'english_title': ''
                }
                
                # 1. Ìè¨Ïä§ÌÑ∞ Ïù¥ÎØ∏ÏßÄ ÌÅ¨Î°§ÎßÅ (ÏÇ¨Ïö©Ïûê Ï†úÍ≥µ Íµ¨Ï°∞)
                try:
                    # ÏÇ¨Ïö©Ïûê Ï†úÍ≥µ: .thumb ._item img
                    thumb_link = soup.find('a', class_='thumb')
                    if not thumb_link:
                        # ÎåÄÏïà ÏÖÄÎ†âÌÑ∞
                        thumb_link = soup.select_one('a[class*="thumb"]')
                    
                    if thumb_link:
                        img_tag = thumb_link.find('img')
                        if img_tag and img_tag.get('src'):
                            poster_url = img_tag['src']
                            result['poster_url'] = optimize_image_50kb(poster_url)
                            print(f"   ‚úÖ Ìè¨Ïä§ÌÑ∞: {result['poster_url'][:50]}...")
                    
                    # Ï∂îÍ∞Ä ÏãúÎèÑ
                    if not result['poster_url']:
                        img_tags = soup.find_all('img')
                        for img in img_tags:
                            src = img.get('src', '')
                            alt = img.get('alt', '')
                            if ('pstatic.net' in src and 'movie' in src) or movie_title in alt:
                                result['poster_url'] = optimize_image_50kb(src)
                                print(f"   ‚úÖ ÎåÄÏïà Ìè¨Ïä§ÌÑ∞ Î∞úÍ≤¨")
                                break
                        
                except Exception as e:
                    print(f"   ‚ùå Ìè¨Ïä§ÌÑ∞ ÌÅ¨Î°§ÎßÅ Ïã§Ìå®: {e}")
                
                # 2. Ï§ÑÍ±∞Î¶¨ ÌÅ¨Î°§ÎßÅ (ÏÇ¨Ïö©Ïûê Ï†úÍ≥µ Íµ¨Ï°∞)
                try:
                    # ÏÇ¨Ïö©Ïûê Ï†úÍ≥µ: .intro_box._content .text._content_text
                    intro_box = soup.find('div', class_='intro_box')
                    if not intro_box:
                        intro_box = soup.select_one('div[class*="intro"]')
                    
                    if intro_box:
                        # ÏÇ¨Ïö©Ïûê Ï†úÍ≥µ: .text._content_text
                        plot_element = intro_box.find('p', class_='text')
                        if not plot_element:
                            plot_element = intro_box.find('p')
                        
                        if plot_element:
                            plot_text = plot_element.get_text().strip()
                            if plot_text and len(plot_text) > 20:
                                # ÌÖçÏä§Ìä∏ Ï†ïÎ¶¨
                                plot_clean = re.sub(r'\s+', ' ', plot_text).strip()
                                if not any(skip_word in plot_clean for skip_word in ['Î°úÍ∑∏Ïù∏', 'ÌöåÏõêÍ∞ÄÏûÖ', 'ÎÑ§Ïù¥Î≤Ñ', 'Í≤ÄÏÉâ']):
                                    result['plot'] = plot_clean
                                    print(f"   ‚úÖ Ï§ÑÍ±∞Î¶¨: {result['plot'][:50]}...")
                                
                except Exception as e:
                    print(f"   ‚ùå Ï§ÑÍ±∞Î¶¨ ÌÅ¨Î°§ÎßÅ Ïã§Ìå®: {e}")
                
                # 3. ÏÉÅÏÑ∏ Ï†ïÎ≥¥ ÌÅ¨Î°§ÎßÅ (ÏÇ¨Ïö©Ïûê Ï†úÍ≥µ Íµ¨Ï°∞)
                try:
                    # ÏÇ¨Ïö©Ïûê Ï†úÍ≥µ: .info.txt_4 .info_group
                    info_area = soup.find('dl', class_='info')
                    if not info_area:
                        info_area = soup.select_one('dl[class*="info"]')
                    
                    if info_area:
                        # ÏÇ¨Ïö©Ïûê Ï†úÍ≥µ: .info_group
                        info_groups = info_area.find_all('div', class_='info_group')
                        if not info_groups:
                            info_groups = info_area.find_all('div')
                        
                        for group in info_groups:
                            try:
                                dt = group.find('dt')
                                dd = group.find('dd')
                                if dt and dd:
                                    label_text = dt.get_text().strip()
                                    value_text = dd.get_text().strip()
                                    
                                    # Ï†ïÎ≥¥ Îß§Ìïë
                                    if 'Ïû•Î•¥' in label_text and value_text:
                                        result['genre'] = value_text
                                        print(f"   ‚úÖ Ïû•Î•¥: {value_text}")
                                    elif 'Îì±Í∏â' in label_text and value_text:
                                        result['grade'] = value_text
                                        print(f"   ‚úÖ Îì±Í∏â: {value_text}")
                                    elif 'Îü¨ÎãùÌÉÄÏûÑ' in label_text and value_text:
                                        result['runtime'] = value_text
                                        print(f"   ‚úÖ Îü¨ÎãùÌÉÄÏûÑ: {value_text}")
                                        
                            except Exception as e:
                                print(f"   ÏÉÅÏÑ∏Ï†ïÎ≥¥ ÌååÏã± Ïò§Î•ò: {e}")
                                continue
                                
                except Exception as e:
                    print(f"   ‚ùå ÏÉÅÏÑ∏Ï†ïÎ≥¥ ÌÅ¨Î°§ÎßÅ Ïã§Ìå®: {e}")
                
                # 4. ÏòÅÏñ¥Ï†úÎ™© ÌÅ¨Î°§ÎßÅ
                try:
                    sub_title_area = soup.find('div', class_='sub_title')
                    if sub_title_area:
                        spans = sub_title_area.find_all('span', class_='txt')
                        for span in spans:
                            text = span.get_text().strip()
                            # ÏòÅÏñ¥ Ï†úÎ™© Ìå®ÌÑ¥ ÌôïÏù∏
                            if re.match(r'^[A-Za-z\s:\'\.\-]+$', text) and len(text) > 2 and text not in ['ÏòÅÌôî', '2025']:
                                result['english_title'] = text
                                print(f"   ‚úÖ ÏòÅÏñ¥Ï†úÎ™©: {text}")
                                break
                                
                except Exception as e:
                    print(f"   ‚ùå ÏòÅÏñ¥Ï†úÎ™© ÌÅ¨Î°§ÎßÅ Ïã§Ìå®: {e}")
                
                # ÌÅ¨Î°§ÎßÅ Í≤∞Í≥º Í≤ÄÏ¶ù
                success_count = sum(1 for v in result.values() if v)
                print(f"‚úÖ '{movie_title}' ÏûêÎèô ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å ({success_count}/6 Ìï≠Î™© ÏÑ±Í≥µ)")
                
                return result
                
            except Exception as e:
                print(f"‚ùå '{movie_title}' Ï†ÑÏ≤¥ ÌÅ¨Î°§ÎßÅ Ïã§Ìå®: {e}")
                return {
                    'poster_url': '',
                    'plot': 'Ï§ÑÍ±∞Î¶¨ Ï†ïÎ≥¥Î•º ÏàòÏßë Ï§ëÏûÖÎãàÎã§.',
                    'genre': 'Ï†ïÎ≥¥ ÏàòÏßë Ï§ë',
                    'runtime': 'Ï†ïÎ≥¥ ÏàòÏßë Ï§ë',
                    'grade': 'Ï†ïÎ≥¥ ÏàòÏßë Ï§ë',
                    'english_title': ''
                }

        def main():
            print('ü§ñ ÏôÑÏ†Ñ ÏûêÎèôÌôî ÌÅ¨Î°§ÎßÅ ÏãúÏä§ÌÖú ÏãúÏûë...')
            
            # KOBIS API Ìò∏Ï∂ú
            kobis_key = os.environ['KOBIS_KEY']
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
            
            print(f'üìÖ Ï°∞Ìöå ÎÇ†Ïßú: {yesterday}')
            
            kobis_url = "http://www.kobis.or.kr/kobisopenapi/webservice/rest/boxoffice/searchDailyBoxOfficeList.json"
            kobis_params = {
                'key': kobis_key,
                'targetDt': yesterday,
                'itemPerPage': 10
            }
            
            kobis_response = requests.get(kobis_url, params=kobis_params, timeout=15)
            kobis_response.raise_for_status()
            kobis_data = kobis_response.json()
            
            movies = kobis_data['boxOfficeResult']['dailyBoxOfficeList']
            print(f'üìä KOBISÏóêÏÑú {len(movies)}Í∞ú ÏòÅÌôî Ï†ïÎ≥¥ ÏàòÏßë ÏôÑÎ£å')
            
            # Í∞Å ÏòÅÌôî ÏôÑÏ†Ñ ÏûêÎèô ÌÅ¨Î°§ÎßÅ
            enhanced_movies = []
            
            for i, movie in enumerate(movies):
                print(f'\nüé¨ {i+1}/{len(movies)} - {movie["movieNm"]} ÏôÑÏ†Ñ ÏûêÎèô Ï≤òÎ¶¨...')
                
                movie_title = movie['movieNm']
                
                # ÏôÑÏ†Ñ ÏûêÎèô ÌÅ¨Î°§ÎßÅ (ÌïòÎìúÏΩîÎî© ÏóÜÏùå)
                movie_details = crawl_naver_auto(movie_title)
                time.sleep(3)  # ÏïàÏ†ÑÌïú ÌÅ¨Î°§ÎßÅ Í∞ÑÍ≤©
                
                # Í∞úÎ¥âÏùº Ìè¨Îß∑ÌåÖ
                formatted_open_date = format_date_korean(movie.get('openDt', ''))
                
                # ÏµúÏ¢Ö Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
                enhanced_movie = {
                    **movie,
                    **movie_details,
                    'formatted_open_date': formatted_open_date
                }
                
                enhanced_movies.append(enhanced_movie)
                print(f'‚úÖ {movie_title} ÏôÑÏ†Ñ ÏûêÎèô Ï≤òÎ¶¨ ÏôÑÎ£å')
            
            # ÏµúÏ¢Ö JSON ÏÉùÏÑ±
            final_data = {
                'updateTime': datetime.now().strftime('%YÎÖÑ %mÏõî %dÏùº %HÏãú %MÎ∂Ñ'),
                'targetDate': yesterday,
                'totalMovies': len(enhanced_movies),
                'dataSource': 'KOBIS + ÎÑ§Ïù¥Î≤Ñ ÏôÑÏ†ÑÏûêÎèôÌÅ¨Î°§ÎßÅ (ÌïòÎìúÏΩîÎî© ÏóÜÏùå)',
                'imageOptimized': '50KB ÏûêÎèôÏµúÏ†ÅÌôî',
                'crawlingMethod': 'ÏÇ¨Ïö©ÏûêÏ†úÍ≥µ HTMLÍµ¨Ï°∞ Í∏∞Î∞ò ÏûêÎèôÌÅ¨Î°§ÎßÅ',
                'movies': enhanced_movies
            }
            
            # JSON Ï†ÄÏû•
            with open('movies.json', 'w', encoding='utf-8') as f:
                json.dump(final_data, f, ensure_ascii=False, indent=2)
                
            print(f'\nüéâ ÏôÑÏ†Ñ ÏûêÎèôÌôî ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å!')
            print(f'üìä Ï¥ù {len(enhanced_movies)}Í∞ú ÏòÅÌôî')
            print(f'üñºÔ∏è Ìè¨Ïä§ÌÑ∞ ÏûêÎèôÏàòÏßë: {sum(1 for m in enhanced_movies if m.get("poster_url"))}Í∞ú')
            print(f'üìù Ï§ÑÍ±∞Î¶¨ ÏûêÎèôÏàòÏßë: {sum(1 for m in enhanced_movies if m.get("plot") and "ÏàòÏßë Ï§ë" not in m["plot"])}Í∞ú')
            print(f'üé≠ ÏÉÅÏÑ∏Ï†ïÎ≥¥ ÏûêÎèôÏàòÏßë: {sum(1 for m in enhanced_movies if m.get("genre") and "ÏàòÏßë Ï§ë" not in m["genre"])}Í∞ú')
            
        if __name__ == "__main__":
            main()
        EOF
        
    - name: Commit and push
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "Full Auto Crawler"
        git add movies.json
        if git diff --staged --quiet; then
          echo "Î≥ÄÍ≤ΩÏÇ¨Ìï≠ ÏóÜÏùå"
        else
          git commit -m "ü§ñ ÏôÑÏ†ÑÏûêÎèôÌôîÌÅ¨Î°§ÎßÅ - ÌïòÎìúÏΩîÎî©ÏóÜÏùå $(date +'%Y-%m-%d %H:%M')"
          git push
          echo "‚úÖ ÏôÑÏ†Ñ ÏûêÎèôÌôî Îç∞Ïù¥ÌÑ∞ ÏóÖÎ°úÎìú"
        fi
